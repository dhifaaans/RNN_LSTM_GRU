{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis-IMDB-RNN-LSTM-GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOaMl2BZL3iqVi6G8p99ZUT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fathanick/RNN_LSTM_GRU/blob/master/Sentiment_Analysis_IMDB_RNN_LSTM_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS-yRSmPQqUX",
        "colab_type": "text"
      },
      "source": [
        "###References\n",
        "\n",
        "*   https://medium.com/@a.ydobon/tensorflow-2-0-text-classification-with-an-rnn-15220b5201b1\n",
        "*   https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.2-understanding-recurrent-neural-networks.ipynb\n",
        "*   https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0pMCerC-NQc",
        "colab_type": "code",
        "outputId": "ed19b0e5-7dde-4275-9b0c-8cf83c9a894a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcywVIHB_JDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR7NP4UBKBdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fix random seed for reproducibility\n",
        "np.random.seed(7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-CxjhopQ2-6",
        "colab_type": "text"
      },
      "source": [
        "###Load IMDB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx_sXfJxLj-M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6c80e7bd-7b47-4caf-e859-4d1a4077a953"
      },
      "source": [
        "# load the dataset but only keep the top n words, zero the rest\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "\n",
        "print(len(X_train), 'train sequences')\n",
        "print(len(X_test), 'test sequences')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 2s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKIEk4-aSz8r",
        "colab_type": "text"
      },
      "source": [
        "####Pad Sequences\n",
        "\n",
        "In order to feed this data into our RNN, all input documents must have the same length. We will limit the maximum review length to max_words by truncating longer reviews and padding shorter reviews with a null value (0). We can accomplish this using the pad_sequences() function in Keras. For now, set max_words to 500."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1zpUKPcSzhb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "93739664-ba17-41b8-c839-80bdc219aff5"
      },
      "source": [
        "print('Pad sequences (samples x time)')\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "max_words = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
        "print('input_train shape:', X_train.shape)\n",
        "print('input_test shape:', X_test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pad sequences (samples x time)\n",
            "input_train shape: (25000, 500)\n",
            "input_test shape: (25000, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFsrtD76dG2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLaXyb48TIug",
        "colab_type": "text"
      },
      "source": [
        "###Simple RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-GjUOkZNcA3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9f3a9498-16d0-46a0-e13d-dcf97130690a"
      },
      "source": [
        "model_RNN = Sequential()\n",
        "model_RNN.add(Embedding(10000, 32))\n",
        "model_RNN.add(SimpleRNN(32))\n",
        "model_RNN.add(Dense(1, activation='sigmoid'))\n",
        "model_RNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "history_RNN = model_RNN.fit(X_train, y_train,\n",
        "                    epochs=5,\n",
        "                    batch_size=512,\n",
        "                    validation_split=0.2)       "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 17s 858us/sample - loss: 0.6637 - acc: 0.5989 - val_loss: 0.5985 - val_acc: 0.7252\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 14s 719us/sample - loss: 0.5130 - acc: 0.7758 - val_loss: 0.5647 - val_acc: 0.7130\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 14s 716us/sample - loss: 0.4828 - acc: 0.7951 - val_loss: 0.4302 - val_acc: 0.8142\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 14s 717us/sample - loss: 0.3368 - acc: 0.8679 - val_loss: 0.3894 - val_acc: 0.8374\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 14s 718us/sample - loss: 0.2702 - acc: 0.8988 - val_loss: 0.3718 - val_acc: 0.8494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deVplup2UOPO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "205a0b1a-f172-4ef8-a011-bef4b60b9d45"
      },
      "source": [
        "loss_RNN, acc_RNN = model_RNN.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f\" % (acc_RNN*100), \"%\")\n",
        "print(\"Loss: %.2f\" % (loss_RNN*100), \"%\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 84.40 %\n",
            "Loss: 37.42 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Wt56P_mcmX2",
        "colab_type": "text"
      },
      "source": [
        "###LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hHVlu-xaZ9I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5c91663f-2d5d-4245-ac35-612695c6acaf"
      },
      "source": [
        "model_LSTM = Sequential()\n",
        "model_LSTM.add(Embedding(10000, 32))\n",
        "model_LSTM.add(LSTM(32))\n",
        "model_LSTM.add(Dense(1, activation='sigmoid'))\n",
        "model_LSTM.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "history_LSTM = model_LSTM.fit(X_train, y_train,\n",
        "                    epochs=5,\n",
        "                    batch_size=512,\n",
        "                    validation_split=0.2)   "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 8s 418us/sample - loss: 0.6591 - acc: 0.6495 - val_loss: 0.5701 - val_acc: 0.7712\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 104us/sample - loss: 0.4488 - acc: 0.8137 - val_loss: 0.3819 - val_acc: 0.8476\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 102us/sample - loss: 0.3063 - acc: 0.8774 - val_loss: 0.3129 - val_acc: 0.8694\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 103us/sample - loss: 0.2419 - acc: 0.9097 - val_loss: 0.2917 - val_acc: 0.8796\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 103us/sample - loss: 0.2119 - acc: 0.9231 - val_loss: 0.2956 - val_acc: 0.8796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAowoPB6ddoM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e7b25555-a090-4945-bc4a-6fdd802123f5"
      },
      "source": [
        "loss_LSTM, acc_LSTM = model_LSTM.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f\" % (acc_LSTM*100), \"%\")\n",
        "print(\"Loss: %.2f\" % (loss_LSTM*100), \"%\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 87.52 %\n",
            "Loss: 30.77 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa3P-_Vsdkw_",
        "colab_type": "text"
      },
      "source": [
        "###GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dtdZP7odluO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7c6dd985-49e1-4a3c-aba3-16e3766d0dc0"
      },
      "source": [
        "model_GRU = Sequential()\n",
        "model_GRU.add(Embedding(10000,32))\n",
        "model_GRU.add(GRU(32))\n",
        "model_GRU.add(Dense(1, activation='sigmoid'))\n",
        "model_GRU.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "history_GRU = model_GRU.fit(X_train, y_train,\n",
        "                    epochs=5,\n",
        "                    batch_size=512,\n",
        "                    validation_split=0.2)   "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 4s 189us/sample - loss: 0.6833 - acc: 0.5901 - val_loss: 0.6568 - val_acc: 0.6484\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 109us/sample - loss: 0.5363 - acc: 0.7452 - val_loss: 0.5066 - val_acc: 0.7502\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 108us/sample - loss: 0.3774 - acc: 0.8353 - val_loss: 0.3795 - val_acc: 0.8310\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 107us/sample - loss: 0.2767 - acc: 0.8880 - val_loss: 0.3593 - val_acc: 0.8536\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 106us/sample - loss: 0.2392 - acc: 0.9081 - val_loss: 0.3466 - val_acc: 0.8606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXG6-S1T3b_h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d45be0e9-bfd9-456c-ab5d-329d1baefac6"
      },
      "source": [
        "loss_GRU, acc_GRU = model_GRU.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f\" % (acc_GRU*100), \"%\")\n",
        "print(\"Loss: %.2f\" % (loss_GRU*100), \"%\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 85.84 %\n",
            "Loss: 34.42 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}